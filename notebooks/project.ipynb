{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a8bfaa",
   "metadata": {},
   "source": [
    "# SKARBOT: Full Social Media Text Analysis Pipeline by Sasha Vujisic and Ashish Kalam\n",
    "This notebook processes social media datasets (Twitter and Reddit) to:\n",
    "- Preprocess and clean the text\n",
    "- Extract keyphrases using KeyBERT\n",
    "- Detect paraphrase matches using Sentence-BERT\n",
    "- Analyze internal repetition (bot detection)\n",
    "- Save a final pipeline summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy sentence-transformers keybert tqdm swifter contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68df743",
   "metadata": {},
   "source": [
    "# --- Import Libraries ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from data_preprocessing import preprocess_dataframe\n",
    "from keyphrase_extraction import extract_from_dataframe\n",
    "from paraphrase_detection import compute_similarity_fast, load_model\n",
    "from repetition_analysis import compute_repetition_statistics, get_match_repetitions\n",
    "from save_pipeline_summary import save_pipeline_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda5baf",
   "metadata": {},
   "source": [
    "# --- Helper Functions ---\n",
    "Ensure necessary folders and file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directories():\n",
    "    os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "def archive_processed_files(archive_name):\n",
    "    processed_dir = \"data/processed\"\n",
    "    archive_dir = f\"data/archives/{archive_name}\"\n",
    "    os.makedirs(archive_dir, exist_ok=True)\n",
    "    for filename in os.listdir(processed_dir):\n",
    "        file_path = os.path.join(processed_dir, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            shutil.move(file_path, os.path.join(archive_dir, filename))\n",
    "    print(f\"Archived all processed files to: {archive_dir}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01406d",
   "metadata": {},
   "source": [
    "# --- Main Pipeline Function ---\n",
    "This function executes the full SKARBOT pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "    CANDIDATE,\n",
    "    REFERENCE,\n",
    "    CANDIDATE_COL,\n",
    "    REFERENCE_COL,\n",
    "    CANDIDATE_DIRECTORY,\n",
    "    REFERENCE_DIRECTORY,\n",
    "    PARAPHRASE_SIMILARITY_THRESHOLD=0.80,\n",
    "    REPETITION_THRESHOLD=0.80\n",
    "):\n",
    "    print(\"\\n==== PIPELINE START ====\")\n",
    "    ensure_directories()\n",
    "\n",
    "    model = load_model(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    print(\"\\nStep 1: Load CSVs\")\n",
    "    candidate_df = pd.read_csv(f\"data/{CANDIDATE_DIRECTORY}/{CANDIDATE}.csv\")\n",
    "    reference_df = pd.read_csv(f\"data/{REFERENCE_DIRECTORY}/{REFERENCE}.csv\")\n",
    "\n",
    "    if CANDIDATE_COL == REFERENCE_COL:\n",
    "        new_candidate_col = CANDIDATE_COL + \"2\"\n",
    "        candidate_df = candidate_df.rename(columns={CANDIDATE_COL: new_candidate_col})\n",
    "        CANDIDATE_COL = new_candidate_col\n",
    "\n",
    "    print(\"CSV Loading Complete.\")\n",
    "\n",
    "    print(\"\\nStep 2: Preprocessing Text\")\n",
    "    clean_candidate_df = preprocess_dataframe(candidate_df, text_column=CANDIDATE_COL)\n",
    "    clean_reference_df = preprocess_dataframe(reference_df, text_column=REFERENCE_COL)\n",
    "\n",
    "    print(\"\\nStep 3: Keyphrase Extraction\")\n",
    "    keyphrased_candidate_df = extract_from_dataframe(clean_candidate_df, text_column=CANDIDATE_COL, model=model)\n",
    "    keyphrased_reference_df = extract_from_dataframe(clean_reference_df, text_column=REFERENCE_COL, model=model)\n",
    "\n",
    "    keyphrased_candidate_df.to_csv(f\"data/processed/keyphrases_{CANDIDATE}.csv\", index=False)\n",
    "    keyphrased_reference_df.to_csv(f\"data/processed/keyphrases_{REFERENCE}.csv\", index=False)\n",
    "\n",
    "    print(\"\\nStep 4: Paraphrase Similarity Matching\")\n",
    "    matches = compute_similarity_fast(\n",
    "        keyphrased_candidate_df,\n",
    "        keyphrased_reference_df,\n",
    "        model,\n",
    "        text_column=CANDIDATE_COL,\n",
    "        ref_column=REFERENCE_COL,\n",
    "        keyphrase_column=\"keyphrases\",\n",
    "        threshold=PARAPHRASE_SIMILARITY_THRESHOLD,\n",
    "        batch_size=32\n",
    "    )\n",
    "    matches_df = pd.DataFrame(matches, columns=[REFERENCE_COL, CANDIDATE_COL, \"similarity\"])\n",
    "    matches_df.to_csv(f\"data/processed/matches_{REFERENCE}_{REFERENCE_COL}_{CANDIDATE}_{CANDIDATE_COL}.csv\", index=False)\n",
    "\n",
    "    print(\"\\nStep 5: Internal Repetition Detection\")\n",
    "    similar_candidates, _ = compute_repetition_statistics(\n",
    "        keyphrased_candidate_df[CANDIDATE_COL].tolist(), model, threshold=REPETITION_THRESHOLD\n",
    "    )\n",
    "    similar_references, _ = compute_repetition_statistics(\n",
    "        keyphrased_reference_df[REFERENCE_COL].tolist(), model, threshold=REPETITION_THRESHOLD\n",
    "    )\n",
    "\n",
    "    similar_candidates_df = pd.DataFrame(similar_candidates, columns=[\"index1\", \"index2\", \"similarity\", CANDIDATE_COL, \"duplicate\"])\n",
    "    similar_references_df = pd.DataFrame(similar_references, columns=[\"index1\", \"index2\", \"similarity\", REFERENCE_COL, \"duplicate\"])\n",
    "\n",
    "    similar_candidates_df.to_csv(f\"data/processed/repetitive_{CANDIDATE_COL}.csv\", index=False)\n",
    "    similar_references_df.to_csv(f\"data/processed/repetitive_{REFERENCE_COL}.csv\", index=False)\n",
    "\n",
    "    print(\"\\nStep 6: Paraphrased Text Repetition Analysis\")\n",
    "    match_repetitions = get_match_repetitions(matches_df, similar_candidates_df, CANDIDATE_COL, \"duplicate\")\n",
    "    match_ref_reps = get_match_repetitions(matches_df, similar_references_df, REFERENCE_COL, \"duplicate\")\n",
    "\n",
    "    pd.DataFrame(match_repetitions, columns=[f\"paraphrased_{CANDIDATE_COL}\", \"repetitions\"]).to_csv(\n",
    "        f\"data/processed/repetitive_matches_{CANDIDATE}_{CANDIDATE_COL}.csv\", index=False\n",
    "    )\n",
    "    pd.DataFrame(match_ref_reps, columns=[f\"paraphrased_{REFERENCE_COL}\", \"repetitions\"]).to_csv(\n",
    "        f\"data/processed/repetitive_matches_{REFERENCE}_{REFERENCE_COL}.csv\", index=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nStep 7: Saving Pipeline Summary\")\n",
    "    save_pipeline_summary(\n",
    "        CANDIDATE, \n",
    "        REFERENCE, \n",
    "        CANDIDATE_COL, \n",
    "        REFERENCE_COL, \n",
    "        matches_df,\n",
    "        clean_candidate_df,\n",
    "        clean_reference_df,\n",
    "        similar_candidates, \n",
    "        similar_references,\n",
    "        match_repetitions,\n",
    "        match_ref_reps,\n",
    "        PARAPHRASE_SIMILARITY_THRESHOLD,\n",
    "        REPETITION_THRESHOLD\n",
    "    )\n",
    "\n",
    "    print(\"\\n==== PIPELINE COMPLETE ====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f11502",
   "metadata": {},
   "source": [
    "# --- Run Example Pipeline ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(\n",
    "    CANDIDATE=\"twitter\",\n",
    "    REFERENCE=\"reddit\",\n",
    "    CANDIDATE_COL=\"tweet\",\n",
    "    REFERENCE_COL=\"title\",\n",
    "    CANDIDATE_DIRECTORY=\"trivial\",\n",
    "    REFERENCE_DIRECTORY=\"trivial\",\n",
    "    PARAPHRASE_SIMILARITY_THRESHOLD=0.80,\n",
    "    REPETITION_THRESHOLD=0.80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79f19f",
   "metadata": {},
   "source": [
    "This project notebook was formatted with the help of ChatGPT."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
